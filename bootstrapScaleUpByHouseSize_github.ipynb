{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b0598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee7434",
   "metadata": {},
   "source": [
    "# Bootstrapping/resampling 403 AND THEN scaling up to 7.5 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11704245",
   "metadata": {},
   "outputs": [],
   "source": [
    "###bootstrap to 403 \n",
    "\n",
    "#import dataframes that will allow us to scale the sample by square footage\n",
    "scale = pd.read_csv(\"\")\n",
    "metaData = pd.read_csv(\"\",\n",
    "                       usecols=['Identifier','Floor_Area__ft2_']).reset_index()\n",
    "\n",
    "\n",
    "identifiers = metaData.Identifier.unique()\n",
    "\n",
    "#####variables######\n",
    "num_homes = 7500000\n",
    "nBoot = 100 #number of times we want to bootstap \n",
    "\n",
    "###########\n",
    "finalFinalDF = pd.DataFrame()\n",
    "\n",
    "for k in range(nBoot):\n",
    "    print(str(k+1))\n",
    "    bootDF = pd.DataFrame()\n",
    "    \n",
    "    newIndex = np.random.choice(metaData['index'],size=len(metaData['index']),replace=True)\n",
    "    unique, counts = np.unique(metaData.index[newIndex], return_counts = True)\n",
    "                                     \n",
    "    countsDF = pd.DataFrame({'indexMerge':unique,'count_scale':counts})\n",
    "    \n",
    "    finalDF = metaData.merge(countsDF,how='left',left_on='index',right_on='indexMerge')                            \n",
    "    finalDF['boostrap_run'] = k+1\n",
    "    finalDF['count_scale'] = finalDF.count_scale.fillna(0)                            \n",
    "\n",
    "    finalDF.to_csv()\n",
    "    print('saved weighting')\n",
    "    finalFinalDF = pd.concat([finalFinalDF,finalDF])\n",
    "\n",
    "    \n",
    "    finalDF = finalDF.set_index('Identifier')\n",
    "    for year in [2016,2017,2018,2019]: \n",
    "        print(str(year))\n",
    "        for hour in [1,2,3,4]:\n",
    "            lolp = pd.read_csv()\n",
    "            \n",
    "            overrideFinal = pd.DataFrame()\n",
    "            noOverrideFinal = pd.DataFrame()\n",
    "    \n",
    "            override= pd.DataFrame()\n",
    "            noOverride = pd.DataFrame()\n",
    "    \n",
    "            count = 0\n",
    "            for low, high, percent in zip(scale.more_than, scale.less_than, scale.percentage):\n",
    "                overrideFilter = pd.DataFrame()\n",
    "                noOverrideFilter = pd.DataFrame()\n",
    "                scaleUp = percent * num_homes\n",
    "        \n",
    "                #get the homes from the metadata with the size we want \n",
    "                sample = metaData[(metaData.Floor_Area__ft2_ >= low) & (metaData.Floor_Area__ft2_ < high)].reset_index() #identifiers in group\n",
    "                sampleHomes = finalDF[(finalDF.count_scale >0)&(finalDF.index.isin(sample.Identifier))] #get weighting from the identifiers \n",
    "                sampleNum = sampleHomes.count_scale.sum() # number of homes in group (i.e. number of times the homes are weighted)\n",
    "\n",
    "\n",
    "                for identifier in sample.Identifier:\n",
    "                    homeDF = pd.read_csv()\n",
    "\n",
    "                    scaleID = finalDF.loc[identifier,'count_scale'] \n",
    "                    \n",
    "                    overrideFilter  = overrideFilter.add(pd.DataFrame(homeDF['observed_impact_kW_mean']*-1*scaleID),fill_value=0)\n",
    "                    noOverrideFilter  = noOverrideFilter.add(pd.DataFrame(homeDF['counterfactual_impact_kW_mean']*-1*scaleID),fill_value=0)\n",
    "        \n",
    "    \n",
    "                noOverride = noOverride.add(pd.DataFrame(noOverrideFilter['counterfactual_impact_kW_mean']/1000 * percent * num_homes/sampleNum),fill_value=0)\n",
    "                override = override.add(pd.DataFrame(overrideFilter['observed_impact_kW_mean']/1000 * percent * num_homes/sampleNum),fill_value=0) \n",
    "        \n",
    "\n",
    "            overrideFinal = lolp.merge(override,how='left',left_index=True,right_index=True)\n",
    "            overrideFinal = overrideFinal.rename(columns={'observed_impact_kW_mean':'delta_override'})\n",
    "            overrideFinal.to_csv()\n",
    "    \n",
    "            noOverrideFinal = lolp.merge(noOverride,how='left',left_index=True,right_index=True)\n",
    "            noOverrideFinal = noOverrideFinal.rename(columns={'counterfactual_impact_kW_mean':'delta_noOverride'})\n",
    "            noOverrideFinal.to_csv()\n",
    "    \n",
    "finalFinalDF.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
